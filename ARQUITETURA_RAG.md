# ğŸ—ï¸ Arquitetura do RAG - Como Funciona o BGE-M3

## ğŸ“Š VisÃ£o Geral do Sistema

Este documento explica **exatamente** como o BGE-M3 (Ollama) estÃ¡ integrado no nosso sistema RAG e qual Ã© o papel de cada componente.

---

## ğŸ¯ O Papel do BGE-M3 no RAG

### âœ… O que o BGE-M3 FAZ (Embeddings):
- **Converte texto em vetores numÃ©ricos** (1024 dimensÃµes)
- **Captura o significado semÃ¢ntico** das palavras
- **Permite buscar por similaridade**, nÃ£o apenas palavras-chave

### âŒ O que o BGE-M3 NÃƒO FAZ:
- **NÃƒO gera respostas** (isso Ã© papel do LLM: GPT-4o/Claude)
- **NÃƒO lÃª ou interpreta** documentos diretamente
- **NÃƒO substitui o LLM** na geraÃ§Ã£o de texto

---

## ğŸ”„ Fluxo Completo do Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FASE 1: INDEXAÃ‡ÃƒO                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. UPLOAD DE DOCUMENTO
   â””â”€> app.py (index_documents)
       â””â”€> document_loader.py
           â†“
   ğŸ“„ Documento.pdf â†’ ğŸ“ Texto extraÃ­do

2. CHUNKING (DivisÃ£o em PedaÃ§os)
   â””â”€> chunker.py
       â†“
   ğŸ“ Texto completo â†’ ğŸ§© 512 chunks de texto

3. GERAÃ‡ÃƒO DE EMBEDDINGS â­ [AQUI ENTRA O BGE-M3]
   â””â”€> embeddings.py (OllamaEmbeddings)
       â†“
   ğŸ§© "Pets sÃ£o proibidos..." â†’ [0.23, -0.87, 0.45, ..., 0.12]
                                 â†‘
                         1024 nÃºmeros (vetor)

4. ARMAZENAMENTO NO FAISS
   â””â”€> vector_store.py
       â†“
   ğŸ’¾ FAISS Index salvou 11 chunks com vetores BGE-M3


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FASE 2: CONSULTA (Query)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. PERGUNTA DO USUÃRIO
   â””â”€> app.py (query_rag)
       â†“
   ğŸ’¬ "Posso ter cachorro?"

2. EMBEDDING DA PERGUNTA â­ [BGE-M3 DE NOVO]
   â””â”€> embeddings.py (embed_query)
       â†“
   ğŸ’¬ "Posso ter cachorro?" â†’ [0.25, -0.85, 0.47, ..., 0.11]
                               â†‘
                       1024 nÃºmeros (mesmo formato!)

3. BUSCA POR SIMILARIDADE
   â””â”€> vector_store.py (search)
       â””â”€> FAISS compara vetores
           â†“
   ğŸ” Vetores mais prÃ³ximos:
       âœ… Chunk 3: "Pets sÃ£o proibidos..." (score: 0.89)
       âœ… Chunk 7: "Animais nÃ£o permitidos..." (score: 0.85)
       âœ… Chunk 2: "Regulamento sobre animais..." (score: 0.78)

4. FORMATAÃ‡ÃƒO DO CONTEXTO
   â””â”€> toon_formatter.py (TOON - reduz tokens 60%)
       â†“
   ğŸ“‹ Contexto formatado com os 3 melhores chunks

5. GERAÃ‡ÃƒO DA RESPOSTA â­ [AQUI ENTRA O LLM: GPT-4o/Claude]
   â””â”€> rag_chain.py
       â””â”€> ChatOpenAI / ChatAnthropic
           â†“
   ğŸ¤– LLM lÃª contexto + pergunta â†’ Gera resposta completa

6. RESPOSTA FINAL
   â””â”€> app.py retorna para o usuÃ¡rio
       â†“
   âœ… "De acordo com o Regulamento Interno, pets nÃ£o sÃ£o 
       permitidos no condomÃ­nio, conforme descrito no 
       documento 391-REGULAMENTO_INTERNO.pdf..."
```

---

## ğŸ”§ Componentes e Responsabilidades

### 1. **embeddings.py** â†’ BGE-M3 (Ollama)
```python
# FunÃ§Ã£o: Transformar texto em vetores
chunk = "Pets sÃ£o proibidos no condomÃ­nio"
vetor = embeddings.embed_query(chunk)
# vetor = [0.23, -0.87, 0.45, ..., 0.12]  (1024 dimensÃµes)
```

**ConfiguraÃ§Ã£o Otimizada:**
- `model: bge-m3` â†’ MultilÃ­ngue, otimizado para retrieval
- `num_ctx: 8192` â†’ Aceita chunks grandes
- `provider: ollama` â†’ Local, grÃ¡tis, privado

---

### 2. **vector_store.py** â†’ FAISS
```python
# FunÃ§Ã£o: Armazenar e buscar vetores
faiss_index.add(vetores_dos_chunks)
resultados = faiss_index.search(vetor_da_pergunta, top_k=8)
```

**O que salva:**
- `index.faiss` â†’ Vetores binÃ¡rios (BGE-M3 1024D)
- `index.pkl` â†’ Metadados (arquivo, chunk, texto)
- `index_metadata.json` â†’ Info do modelo usado

---

### 3. **rag_chain.py** â†’ LLM (GPT-4o/Claude)
```python
# FunÃ§Ã£o: Ler contexto e gerar resposta
prompt = f"""
Contexto: {chunks_recuperados}
Pergunta: {pergunta_usuario}
"""
resposta = llm.invoke(prompt)
```

**NÃƒO usa BGE-M3** â†’ Usa GPT-4o ou Claude Sonnet

---

## âš ï¸ IMPORTANTE: ConsistÃªncia de Embeddings

### âŒ PROBLEMA se misturar modelos:
```
Indexou com: BGE-M3 (1024 dimensÃµes)
Buscar com:  OpenAI  (1536 dimensÃµes)
â†’ ERRO! DimensÃµes incompatÃ­veis!
```

### âœ… SOLUÃ‡ÃƒO implementada:
1. **Salvamos o modelo usado** em `index_metadata.json`
2. **Mostramos na UI** qual modelo estÃ¡ ativo
3. **Reindexamos** ao trocar de modelo

---

## ğŸ¨ Por que BGE-M3 Ã© Superior?

| Aspecto | BGE-M3 (Ollama) | OpenAI Embeddings |
|---------|-----------------|-------------------|
| **Custo** | R$ 0,00 (local) | ~R$ 0,02/1M tokens |
| **Privacidade** | 100% local | Envia dados para API |
| **PortuguÃªs** | Excelente (treino multilÃ­ngue) | Bom (focado inglÃªs) |
| **Retrieval** | Otimizado para busca | GenÃ©rico |
| **DimensÃµes** | 1024 (eficiente) | 1536/3072 (maior) |
| **Velocidade** | 0.5s/embedding | ~0.3s/embedding |
| **Nuances** | Captura sinÃ´nimos PT-BR | Pode falhar em expressÃµes BR |

### ğŸ“ˆ Exemplo Real:

**Pergunta:** *"Posso ter animal de estimaÃ§Ã£o?"*

**BGE-M3 encontra:**
- "Pets sÃ£o proibidos" âœ…
- "Animais nÃ£o permitidos" âœ…  
- "CÃ£es e gatos vedados" âœ…

**OpenAI pode falhar em:**
- "Bichinhos nÃ£o liberados" âŒ (expressÃ£o coloquial BR)
- "Cachorrinho proibido" âŒ (diminutivo portuguÃªs)

---

## ğŸ” Como Validar se EstÃ¡ Funcionando?

### 1. Verifique o log de indexaÃ§Ã£o:
```
ğŸ“‚ Contexto: cond_391
ğŸ”§ Modelo de Embeddings: bge-m3 (OLLAMA)
âœ… 1 arquivo(s) indexado(s):
  â€¢ 391-REGULAMENTO_INTERNO.pdf
```

### 2. Teste similaridade semÃ¢ntica:
```python
# Pergunta com sinÃ´nimo
pergunta = "Posso ter cachorro?"

# Deve encontrar chunks com:
# - "pets proibidos"
# - "animais nÃ£o permitidos"
# - "cÃ£es vedados"
```

### 3. Compare respostas:
- **Com BGE-M3:** Respostas mais precisas, encontra trechos relevantes
- **Sem BGE-M3:** Pode errar buscas por palavras diferentes

---

## ğŸ“ Estrutura de Dados

```
data/faiss_index/
â”œâ”€â”€ cond_391/
â”‚   â”œâ”€â”€ index.faiss         # Vetores BGE-M3 (1024D cada)
â”‚   â”œâ”€â”€ index.pkl           # Metadados dos chunks
â”‚   â””â”€â”€ index_metadata.json # Info: modelo=bge-m3, provider=ollama
```

**ConteÃºdo de `index_metadata.json`:**
```json
{
  "indexed_files": ["391-REGULAMENTO_INTERNO.pdf"],
  "indexed_at": "2025-12-12T13:35:28",
  "total_files": 1,
  "embedding_model": {
    "provider": "OllamaEmbeddings",
    "model": "bge-m3"
  }
}
```

---

## ğŸš€ Resumo: O que BGE-M3 Realmente Faz?

1. **Na INDEXAÃ‡ÃƒO:**
   - LÃª cada chunk de texto
   - Transforma em vetor de 1024 nÃºmeros
   - FAISS salva esses vetores

2. **Na CONSULTA:**
   - LÃª a pergunta do usuÃ¡rio
   - Transforma em vetor de 1024 nÃºmeros
   - FAISS compara com vetores salvos
   - Retorna chunks mais similares

3. **Depois:**
   - LLM (GPT-4o/Claude) lÃª os chunks
   - LLM gera a resposta final
   - BGE-M3 jÃ¡ fez sua parte (retrieval)

---

## âœ… Checklist de ImplementaÃ§Ã£o Correta

- [x] BGE-M3 usado na **indexaÃ§Ã£o** (embed_documents)
- [x] BGE-M3 usado na **busca** (embed_query)
- [x] FAISS armazena vetores BGE-M3
- [x] Metadados salvam modelo usado
- [x] UI mostra qual modelo estÃ¡ ativo
- [x] ParÃ¢metros otimizados (num_ctx=8192)
- [x] LLM separado do embedding (GPT-4o/Claude)
- [x] Sistema de contextos (cond_391, etc)

---

## ğŸ¯ ConclusÃ£o

**O BGE-M3 estÃ¡ PERFEITAMENTE implementado no seu RAG!**

Ele faz **exatamente** o que deve fazer:
- âœ… Gerar embeddings semÃ¢nticos de alta qualidade
- âœ… Permitir busca por similaridade, nÃ£o palavras exatas
- âœ… Funcionar de forma local, rÃ¡pida e gratuita
- âœ… Otimizado para portuguÃªs e retrieval

**NÃ£o precisa de ajustes na funÃ§Ã£o** â€” apenas aproveitar! ğŸ‰

---

ğŸ“… **Ãšltima atualizaÃ§Ã£o:** 12/12/2025  
ğŸ”§ **VersÃ£o:** 1.0.0  
ğŸ‘¨â€ğŸ’» **Autor:** Sistema RAG Multi-Contexto
